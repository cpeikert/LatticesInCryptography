\documentclass[11pt]{article}

\input{header}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% VARIABLES

\newcommand{\lecturenum}{3}
\newcommand{\lecturetopic}{LLL Algorithm}
\newcommand{\scribename}{Yan Wang}

% END OF VARIABLES

\lecheader                      % execute lecture commands

\pagestyle{plain}               % default: no special header

\begin{document}

\thispagestyle{fancy} % first page should have special header

% LECTURE MATERIAL STARTS HERE

\section{The LLL Algorithm}%
\label{sec:lll-algorithm}

Recall the definition of an LLL-reduced lattice basis, as defined by Lenstra, Lenstra, and Lov{\'a}sz~\cite{lenstra82:_factor}, and how it approximates a short nonzero lattice vector.
(In all that follows, recall that $\matB = \gs{\matB} \cdot \matU$ is the Gram--Schmidt decomposition of a lattice basis~$\matB$, where the columns of~$\gs{\matB}$ are orthogonal and $\matU \in \R^{n \times n}$ is upper unitriangular, with entries~$\mu_{i,j}$.)

\begin{definition}%
  \label{def:lll}
  A lattice basis $\matB$ is \emph{LLL-reduced} if the following two conditions are met:
  \begin{enumerate}
  \item $\abs{\mu_{i,j}} \leq \frac{1}{2}$ for all $i < j$.\label{item:size}
    %
    \hfill (Such a basis is said to be ``size reduced.'')
    
  \item $\frac{3}{4}\length{ \gs{\vecb}_i}^2 \leq \length{\mu_{i,i+1}\gs{\vecb}_i + \gs{\vecb}_{i+1}}^2$ for all $i < n$.\label{item:lovasz}
    %
    \hfill (This is the ``Lov{\'a}sz condition.'')
  \end{enumerate}
\end{definition}

\begin{lemma}%
  \label{lem:lll-gs}
  In an LLL-reduced lattice basis $\matB \in \Z^{n \times n}$, we have that $\length{\gs{\vecb}_{i+1}}^{2} \geq \frac12 \length{\gs{\vecb}_{i}}^{2}$ for all $i < n$, and hence $\length{\vecb_1} \leq 2^{(n-1)/2} \min_{i} \length{\gs{\vecb}_{i}} \leq 2^{(n-1)/2} \cdot \lambda_{1}(\lat(\matB))$.
\end{lemma}

\Cref{alg:lll} defines the LLL algorithm, which transforms any lattice basis into an LLL-reduced one of the same lattice.
The algorithm simply alternates between two steps: \emph{size-reducing} the current basis (i.e., making \cref{item:size} hold), and checking whether the Lov{\'a}sz condition (\cref{item:lovasz}) holds; if not, it simply swaps a pair of offending basis vectors.
From this description, we can immediately see that the output is indeed LLL-reduced, so the algorithm is correct---if it ever terminates.
The key challenge is in showing that it does indeed terminate, and in time polynomial in the input size.

\begin{algorithm}
  \caption{The LLL algorithm.}%
  \label{alg:lll}

  \begin{algorithmic}[1]
    \zcsetup{reftype=line}
    
    \Function{\algo{LLL}}{$\matB \in \Z^{n \times n}$}
    \Comment{Outputs an LLL-reduced basis of the lattice
      $\lat(\matB)$}

    \State let
    $\matB \gets \algo{SizeReduce}(\matB)$\label{step:size-reduce}

    \If{there exists an $i < n$ for which the Lov{\'a}sz condition is violated}

    \State swap~$\vecb_{i}$ and~$\vecb_{i+1}$ and go to \cref{step:size-reduce}\label{step:swap}

    \EndIf

    \State \Return $\matB$
    
    \EndFunction

    \Function{\algo{SizeReduce}}{$\matB \in \Z^{n \times n}$}
    \Comment{Outputs a size-reduced basis of $\lat(\matB)$, preserving
      $\gs{\matB}$}

    \State compute (or update) the Gram--Schmidt
    orthogonalization~$\gs{\matB}$ of~$\matB$\label{step:compute-gs}

    \For{$j = 2, \ldots, n$ (in any order)}
    \For{$i = j-1 \text{ down to } 1$}\label{step:i-downto}

    \State let
    $\vecb_{j} \gets \vecb_{j} - \round{u_{i,j}} \cdot \vecb_{i}$,
    where
    $u_{i,j} = \inner{\vecb_{j},
      \gs{\vecb}_{i}}/\inner{\gs{\vecb}_{i}, \gs{\vecb}_{i}}$

    \EndFor \EndFor \EndFunction
  \end{algorithmic}
\end{algorithm}

The $\algo{SizeReduce}(\matB)$ subroutine deserves some elaboration.
Its purpose is, in the Gram--Schmidt decomposition $\matB = \gs{\matB} \cdot \matU$, to shift the entries in the upper triangle of~$\matU$ by integers, so that they lie in $\hohalf$.
Because we must preserve the lattice, this is done by subtracting appropriate integer multiples of the basis vectors~$\vecb_{i}$ (\emph{not} the Gram--Schmidt vectors~$\gs{\vecb}_{i}$!)
from the~$\vecb_{j}$, for $i < j$.
In matrix form, the $(i,j)$th iteration lets $\matB \gets \matB \cdot \matW = \gs{\matB} \cdot (\matU \matW)$, where $\matW$ is the upper unitriangular matrix with just one possibly nonzero off-diagonal entry $-\round{u_{i,j}}$, at position $(i,j)$.
This implicitly updates the (upper unitriangular) matrix $\matU \gets \matU \matW \in \R^{n \times n}$.
This matrix is identical to~$\matU$, except possibly at position $(i,j)$ \emph{and the entries above it, but not below it} in the same column.
This is why we need to make the changes going \emph{upward} in each column (see \cref{step:i-downto}).

\begin{lemma}%
  \label{lem:size-reduce-correct}
  Given an integral lattice basis $\matB \in \Z^{n \times n}$ with Gram--Schmidt orthogonalization $\gs{\matB}$, the $\algo{SizeReduce}$ algorithm outputs a basis~$\matB'$ of $\lat = \lat(\matB)$ having Gram--Schmidt decomposition $\matB' = \gs{\matB} \cdot \matU'$, where $u'_{i,j} \in \hohalf$ for all $i<j$.
\end{lemma}

\begin{proof}
  First, even though $\algo{SizeReduce}$ may change~$\matB$, the Gram--Schmidt vectors~$\gs{\matB}$ are preserved throughout, because the only changes to~$\matB$ are via multiplication by upper-unitriangular matrices, i.e., if $\matB = \gs{\matB} \cdot \matU$ is the Gram--Schmidt decomposition prior to some iteration, then $\matB = \gs{\matB} \cdot (\matU \matW)$ is the decomposition afterward, since $\matU \matW$ is upper unitriangular.
  Second, the $(i,j)$th iteration ensures that the value $\inner{\vecb_{j}, \gs{\vecb}_{i}}/\inner{\gs{\vecb}_{i}, \gs{\vecb}_{i}} \in \hohalf$, and that value never changes in later iterations, because for all $h < i$, the basis vector~$\vecb_{h}$ (a multiple of which may later be subtracted from~$\vecb_{j}$) is orthogonal to~$\gs{\vecb}_{i}$.
\end{proof}

\noindent We now state the main theorem about the LLL algorithm.

\begin{theorem}%
  \label{thm:lll}
  Given an integral lattice basis $\matB \in \Z^{n \times n}$, the LLL algorithm outputs an LLL-reduced basis of $\lat = \lat(\matB)$ in time $\poly(n, \len{\matB})$, where $\len{\matB}$ denotes the bit length of the input basis.
\end{theorem}

The remainder of this section is dedicated to an (almost complete) proof of this theorem.
First, it is clear that the LLL algorithm, if it ever terminates, is correct: all the operations on the input basis preserve the lattice it generates, and the algorithm can only output an LLL-reduced basis.

We next prove that the number of iterations is $O(N)$ for some $N=\poly(n, \len{\matB})$.
This uses a ``potential argument,'' which assigns a value to all the intermediate bases produced by the algorithm.
We show three facts: that the potential starts out no larger than~$2^{N}$, that it never drops below~$1$, and that each iteration of the algorithm decreases the potential by a factor of at least $\sqrt{4/3} > 1$.
All this implies that the number of iterations is at most $\log_{\sqrt{4/3}} 2^{N} = O(N)$.

The potential function is defined as follows: for a basis $\matB = (\vecb_{1}, \ldots, \vecb_{n})$, let $\lat_{i} = \lat(\vecb_{1}, \ldots, \vecb_{i})$ for each $i \leq n$.
The potential is the product of the determinants of these lattices:
\begin{equation}
  \label{eq:potential}
  \Phi(\matB) := \prod_{i=1}^{n} \det(\lat_{i}) = \prod_{i=1}^{n}
  \parens*{\length{\gs{\vecb}_{1}} \cdots \length{\gs{\vecb}_{i}}} =
  \prod_{i=1}^{n} \length{\gs{\vecb}_{i}}^{n-i+1}.
\end{equation}

\begin{claim}%
  \label{clm:potential}
  The potential of the input basis~$\matB$ is at most~$2^{N}$, and every intermediate basis the algorithm produces has potential at least~$1$.
\end{claim}

\begin{proof}
  Because $\length{\gs{\vecb}_{i}} \leq \length{\vecb_{i}}$ and $\length{\vecb_{i}} \geq 1$ for all~$i$, the potential of the input basis~$\matB$ is bounded by
  \[ \Phi(\matB) \leq \prod_{i=1}^{n} \length{\vecb_{i}}^{n-i+1} \leq \prod_{i=1}^{n} \length{\vecb_{i}}^{n} \leq \max_{i} \length{\vecb_{i}}^{n^{2}} = 2^{\poly(n,\len{\matB})}.
  \] Every intermediate basis is integral, hence the lattices~$\lat_{i}$ associated with that basis have determinant at least~$1$.\footnote{This requires some care to verify, because the lattices~$\lat_{i}$ are \emph{not full rank} for $i < n$.
    Letting $\matB_{i} = (\vecb_{1}, \ldots, \vecb_{i}) \in \Z^{n \times i}$ be a basis for~$\lat_{i}$, we have $\det(\lat_{i}) = \sqrt{\det(\matB_{i}^{t} \matB_{i})} \geq 1$, because the Gram matrix $\matB_{i}^{t} \matB_{i} \in \Z^{i \times i}$ is integral and nonsingular.}
  Therefore, the potential of that basis is at least~$1$.
\end{proof}

\noindent We next analyze how the potential changes when we perform a swap in \cref{step:swap}.

\begin{claim}%
  \label{clm:swap-gs}
  Suppose that~$\vecb_{i}$ and~$\vecb_{i+1}$ are swapped in \cref{step:swap}, and let the resulting basis be denoted~$\matB'$.
  Then $\gs{\vecb}'_{j} = \gs{\vecb}_{j}$ for all $j \not\in \set{i,i+1}$, and $\gs{\vecb}'_{i} = \mu_{i,i+1} \gs{\vecb}_{i} + \gs{\vecb}_{i+1}$.
\end{claim}

Note that this claim lets us optimize \cref{step:compute-gs}: following a swap, we need not orthogonalize~$\matB'$ from scratch, but only need to update two of the vectors from~$\gs{\matB}$.

\begin{proof}
  For all $j < i$, we have $\gs{\vecb}'_{j} = \gs{\vecb}_{j}$, because it is the component of $\vecb'_{j} = \vecb_{j}$ orthogonal to $\spn(\vecb'_{1}, \ldots, \vecb'_{j-1}) = \spn(\vecb_{1}, \ldots, \vecb_{j-1})$.
  Similarly, for any $j > i+1$, we have $\gs{\vecb}'_{j} = \gs{\vecb}_{j}$, because it is the component of $\vecb'_{j} = \vecb_{j}$ orthogonal to $\spn(\vecb'_{1}, \ldots, \vecb'_{j-1}) = \spn(\vecb_{1}, \ldots, \vecb_{j-1})$, where here the equality holds because both~$\vecb_{i}$ and~$\vecb_{i+1}$ are in the lists of arguments (in reversed order).
  Finally, $\gs{\vecb}'_{i}$ is the component of $\vecb'_{i} = \vecb_{i+1}$ orthogonal to $\spn(\vecb'_{1}, \ldots, \vecb'_{i-1}) = \spn(\vecb_{1}, \ldots, \vecb_{i-1})$, which is $\mu_{i,i+1} \gs{\vecb}_{i} + \gs{\vecb}_{i+1}$ by construction.
\end{proof}

\begin{lemma}%
  \label{lem:lll-potential}
  Suppose that~$\vecb_{i}$ and~$\vecb_{i+1}$ are swapped in \cref{step:swap}, and let the resulting basis be denoted~$\matB'$.
  Then $\Phi(\matB')/\Phi(\matB) < \sqrt{3/4}$.
\end{lemma}

\begin{proof}
  Let $\lat_i = \lat( \vecb_1, \ldots , \vecb_{i-1}, \vecb_i ) $ and $\lat_i' = \lat( \vecb_1, \ldots , \vecb_{i-1}, \vecb_{i+1} ) $.
  By \cref{clm:swap-gs},
  \begin{equation}
    \begin{split}
      \frac{\Phi(\matB')}{\Phi(\matB)} =
      \frac{\det(\lat_i')}{\det(\lat_i)} =
      \frac{\length{\gs{\vecb}_{1}} \cdots \length{\gs{\vecb}_{i-1}}
      \length{\mu_{i,i+1} \gs{\vecb}_{i} + \gs{\vecb}_{i+1}}
      }{\length{\gs{\vecb}_{1}} \cdots \length{\gs{\vecb}_{i-1}}
      \length{\gs{\vecb}_{i}}} = \frac{\length{\mu_{i,i+1}
      \gs{\vecb}_{i} + \gs{\vecb}_{i+1}}}{\length{\gs{\vecb}_{i}}}
      < \sqrt{3/4},
    \end{split}
  \end{equation}
  where the last inequality follows from the fact that the swap occurs because the Lov{\'a}sz condition (\cref{item:lovasz}) is \emph{not} satisfied.
\end{proof}

This completes the proof that the number of iterations is $O(N) = \poly(n, \len{\matB})$.
Moreover, we can see by inspection that each iteration of the algorithm is polynomial time in the bit length of the current basis.
However, this does \emph{not} necessarily guarantee that the LLL algorithm is polynomial time overall, since the bit length of the intermediate bases could possibly \emph{increase} with each iteration.
(For example, if the bit length doubled in each iteration, then by the end, the bit length would be exponential in~$n$.)

To complete the full proof that LLL is polynomial time, it suffices to show that the sizes of \emph{all} intermediate bases are some fixed polynomial in the size of the original basis.
This turns out to be the case, specifically due to the size-reduction step (which we have not used up to this point!).
The proof of this fact is somewhat grungy, though, so we won't cover it.

We conclude with some final remarks about the LLL algorithm.
The factor $3/4$ in the Lov{\'a}sz condition is just for convenience of analysis.
We can use any constant between $1/4$ and $1$, which yields a tradeoff between the final approximation factor and the number of iterations, but these will still remain exponential and polynomial (in~$n$), respectively.
By choosing a value very close to~$1$, we can obtain an approximation factor of $(2/\sqrt{3})^{n}$ in polynomial time, but we cannot do any better using LLL.\@ We can get slightly better approximation factors of $2^{O(n(\log\log n)^2/\log n)}$, still in polynomial time, using Schnorr's generalization~\cite{DBLP:journals/tcs/Schnorr87} of LLL, where the analogue of the Lov{\'a}sz condition deals with blocks of $k \geq 2$ consecutive vectors.

\bibliography{common/lattices}
\bibliographystyle{common/alphaabbrvprelim}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
