\documentclass[11pt]{article}

\input{header}

% VARIABLES

\newcommand{\lecturenum}{4}
\newcommand{\lecturetopic}{Coppersmith, Cryptanalysis}
\newcommand{\scribename}{Jacob Alperin-Sheriff}

\DeclareMathOperator{\res}{res}

% END OF VARIABLES

\lecheader                      % execute lecture commands

\pagestyle{plain}               % default: no special header

\begin{document}

\thispagestyle{fancy} % first page should have special header

% LECTURE MATERIAL STARTS HERE

\section{Coppersmith's Theorem}%
\label{sec:coppersmith}

One nice application of the LLL algorithm is a technique of Coppersmith~\cite{DBLP:conf/eurocrypt/Coppersmith96} that finds all \emph{small} roots of a polynomial modulo a given number~$N$, even when the factorization of~$N$ is unknown.
This technique has been a very powerful tool in cryptanalysis, as we will see next time.
Here is one important theorem (among many other results) that can be shown using Coppersmith's method.

\begin{theorem}%
  \label{thm:coppersmith}
  There is a polynomial-time algorithm that, given any monic, degree-$d$ integer polynomial $f(x) \in \Z[x]$ and an integer $N$, outputs all integers $x_{0}$ such that $\abs{x_{0}} \leq B = N^{1/d}$ and $f(x_{0}) = 0 \bmod N$.
\end{theorem}

\noindent We make a few important remarks about the various components of this theorem:
\begin{enumerate}[itemsep=0pt]
\item When $N$ is prime, i.e., $\Z_{N}$ is a finite field, there are efficient algorithms that output \emph{all} roots of a given degree-$d$ polynomial $f(x)$ modulo $N$, of which there are at most $d$.
  Similarly, there are efficient algorithm that factor polynomials over the rationals (or integers).
  Therefore, the fact that the theorem handles a composite modulus $N$ is a distinguishing feature.
\item For composite $N$, the number of roots of $f(x)$ modulo~$N$ can be nearly exponential in the bit length of~$N$, even for quadratic $f(x)$.
  For example, if~$N$ is the product of~$k$ distinct primes, then any square modulo~$N$ has exactly~$2^{k}$ distinct square roots.
  (This follows from the Chinese Remainder Theorem, since there are two square roots modulo each prime divisor of $N$.)
  Since~$k$ can be as large as $\approx \log N / \log \log N$, the number of roots can be nearly exponential in $\log N$.
  Therefore, in general no efficient algorithm can output \emph{all} roots of $f(x)$ modulo~$N$; the restriction to \emph{small} roots in the theorem statement circumvents this problem.\footnote{Indeed, the theorem implies that the number of small roots is always polynomially bounded.
    This fact did not appear to be known before Coppersmith's result!}
\item The size restriction appears necessary for another reason: knowing two square roots $r_{1} \neq \pm r_{2}$ of a square modulo a composite~$N$ reveals a nontrivial factor of~$N$, as $\gcd(r_{1}-r_{2}, N)$.
  So even if the number of roots is small, finding all of them is still at least as hard as factoring.
  However, it is easy to show that a square cannot have more than one ``small'' square root, of magnitude at most $N^{1/2}$.
  Therefore, the theorem does not appear to yield an efficient factoring algorithm.
  (However, it can be used to factor when some partial information about a factor is known, as shown in the related work~\cite{DBLP:conf/eurocrypt/Coppersmith96a}.)
\end{enumerate}

\subsection{Warm-Up: Proof for a Weaker Bound}%
\label{sec:warm-up}

To highlight the heart of the method, as a warm-up we prove \cref{thm:coppersmith} for the substantially weaker bound of $B \approx N^{2/(d(d+1))}$, where the approximation hides a small constant factor.
(We will prove the theorem for the bound $B \approx N^{1/d}$ in \cref{sec:proof-claimed-bound} below.)
The strategy is to find another nonzero polynomial $h(x) = \sum h_{i} x^{i} \in \Z[x]$ such that:
\begin{enumerate}[itemsep=0pt]
\item every root of $f(x)$ modulo~$N$ is also a root of $h(x)$ (not modulo anything), and
\item the polynomial $h(Bx)$ is ``short,'' i.e., $\abs{h_{i} B^{i}} < N/(\deg(h)+1)$ for all $i$.
\end{enumerate}
For any such $h(x)$, and for any $x_{0}$ such that $|x_0| \leq B$, we have that $\abs{h_i x_0^i} \leq \abs{h_{i} B^{i}} < N/(\deg(h)+1)$, which implies that $\abs{h(x_0)} < N$.
Hence, for every \emph{small} root~$x_0$ (such that $\abs{x_{0}} \leq B$) of $f(x)$ modulo~$N$, we have that $h(x_0)=0$ \emph{over the integers} (not modulo anything).
To find the small roots of $f(x)$ modulo~$N$, we can therefore just factor $h(x)$ over the integers, and test whether each of its (small) roots is a root of $f(x)$ modulo~$N$.

We now give an efficient algorithm to find such an $h(x)$.
The basic idea is that adding integer multiples of the polynomials $g_{i}(x) = N x^i \in \Z[x]$ to $f(x)$ preserves the roots of~$f$ modulo~$N$.
So, we construct a lattice whose basis vectors correspond to the coefficient vectors of the polynomials $f(Bx)$ and $g_{i}(Bx)$, then find a short nonzero vector in this lattice, and interpret it as a polynomial $h(Bx)$.
Specifically, the lattice basis is
\[ \matB =
  \begin{pmatrix}
    N & & & & f_0 \\
      & B N & & & f_1 B \\
      & & B^2 N & & f_2 B^2   \\
      & & & & \\
      & & & B^{d-1} N & f_{d-1} B^{d-1} \\
      & & & & B^d
  \end{pmatrix}. \]
Notice that the lattice dimension is $d+1$, and that
$\det(\matB) = B^{d(d+1)/2} \cdot N^d$.
By running the LLL algorithm
on this basis, we obtain a nonzero lattice vector~$\vecv \in
\lat(\matB)$ for which
\[ \length{\vecv} \leq 2^{d/2} \cdot B^{d/2} \cdot N^{d/(d+1)} =
  (2B)^{d/2} \cdot N^{1-1/(d+1)}.
\]

Define $h(Bx)$ to be the polynomial whose coefficients are given by $\vecv$, i.e., \[ h(x) = v_0 + (v_{1}/B)x + \cdots + (v_{d}/B^{d}) x^d .
\]
Notice that $h(x) \in \Z[X]$ because $B^i$ divides $v_i$ for each $i$ by construction of the lattice basis, and that every root of $f(x)$ modulo $N$ is also a root of $h(x)$ by construction.
Finally, we see that
\[ \abs{ h_{i} B^{i} } = \abs{v_{i}} \leq \length{\vecv} < \frac{N}{d+1} \] if we take $(2B)^{d/2} < N^{1/(d+1)}/(d+1)$, which is equivalent to $B < N^{2/(d(d+1))}/(2 (d+1)^{2/d})$; note that the denominator is upper bounded by a constant.
This concludes the proof of the warm-up.

\subsection{Proof for the Claimed Bound}%
\label{sec:proof-claimed-bound}

We now (almost) prove \cref{thm:coppersmith}, but for a bound of $B \approx N^{1/d}$, where $\approx$ hides factors that are polynomial in~$d$ and $N^{\varepsilon}$ for any arbitrarily small constant $\varepsilon > 0$.
This yields a true bound of $B = N^{1/d-\varepsilon}$ for any constant $\varepsilon > 0$, and with a bit more grungy work the theorem can be proved for $B=N^{1/d}$ exactly.

Recall that above we considered adding multiples of $N \cdot x^{i}$ to~$f(x)$, which preserves the roots of $f(x)$ modulo~$N$.
This let us obtain a bound of only $B \approx N^{2/d^{2}}$.
To do better, we consider \emph{higher powers} of~$f(x)$ and~$N$.
More specifically, our strategy is to use LLL to efficiently find a nonzero polynomial $h(x) = \sum_{i} h_{i} x^{i} \in \Z[x]$ of degree at most $n = d(m+1)$, for some positive integer~$m$ to be determined, such that:
\begin{enumerate}[itemsep=0pt]
\item Any root of~$f$ modulo~$N$ is a root of~$h$ modulo~$N^{m}$, i.e., if $f(x_{0}) = 0 \bmod N$ then $h(x_{0}) = 0 \bmod N^{m}$.
\item The polynomial $h(Bx)$ is ``short,'' i.e., its coefficients $h_{i} B^{i}$ are all less than $N^{m}/(n+1)$ in magnitude.
\end{enumerate}
From the second property, it follows that if $\abs{x_{0}} \leq B$, then
\[ \abs{h(x_{0})} \leq \sum_{i=0}^{n} \abs{h_{i} B^{i}} < N^{m}.
\] Therefore, by the first property, any small root of~$f$ modulo~$N$ is a root of~$h(x)$ \emph{over the integers} (not modulo anything).
So, having found~$h(x)$, we can efficiently factor it over the integers and check whether each of its small roots is indeed a root of~$f(x)$ modulo~$N$.

The first helpful insight is that $f(x_{0}) = 0 \bmod N$ implies that $f(x_{0})^{k} = 0 \bmod N^{k}$ for any positive integer~$k$.
So, we can define $n=d(m+1)$ polynomials $g_{u,v}(x)$ whose roots modulo~$N^{m}$ will include all the roots of $f(x)$ modulo~$N$, as follows:
\[ g_{u,v}(x)=N^{m-v} \cdot x^{u} \cdot f(x)^{v} \quad \text{for } u \in \set{0,\ldots,d-1}, v \in \set{0,\ldots,m}.
\] We use two important facts about these polynomials:
\begin{itemize}
\item First, $g_{u,v}(x)$ has leading coefficient $N^{m-v}$ and is of degree exactly $u+vd$, because $f(x)$ is monic and of degree $d$.
\item Second, if~$x_{0}$ is a root of $f(x)$ modulo~$N$, then~$x_{0}$ is a root of $g_{u,v}(x)$ modulo $N^{m}$ for all $u,v$, because~$N^{m}$ divides $N^{m-v} f(x_{0})^{v}$.
\end{itemize}
 
The basis vectors for our lattice are coefficient vectors of $g_{u,v}(Bx)$.
In other words, the basis is $\matB=[\vecb_0,\ldots,\vecb_{n-1}]$, where~$\vecb_{u+vd}$ is the coefficient vector of $g_{u,v}(Bx)$ as a polynomial in~$x$.
Since $g_{u,v}(x)$ is of degree $u+vd$ with leading coefficient $N^{m-v}$, and $u+vd$ runs over $\set{0, \ldots, n-1}$ as $u,v$ run over their respective ranges, the basis is triangular, with diagonal entries $N^{m-v} B^{u+vd}$.
A straightforward calculation then reveals that
\begin{equation}
  \label{eq:det-B}
  \det(\matB) = B^{n(n-1)/2} \cdot N^{d m (m+1)/2}.
\end{equation}

Running the LLL algorithm on $\matB$ yields a nonzero vector $\vecv \in \lat(\matB)$ of length
\begin{align}
  \length{\vecv} \leq 2^{(n-1)/2} \det(\matB)^{1/n}
  &= 2^{(n-1)/2} \parens*{B^{n(n-1)/2} \cdot N^{d m(m+1)/2}}^{1/n} \\
  &\leq (2 B)^{n/2} \cdot N^{m/2}.
\end{align}
Setting $B \approx (N^{1/d})^{m/(m+1)}$, which is $N^{1/d-\varepsilon}$ for large enough (but still polynomially bounded) $m$, we can ensure that $(2B)^{n/2} < N^{m/2}/(n+1)$ and therefore that $\length{\vecv} < N^{m}/(n+1)$, as required.
Reading off the entries of $\vecv$ as the coefficients of $h(Bx)$ yields a satisfactory polynomial $h(x)$.

\section{Cryptanalysis of RSA Variants}%
\label{sec:crypt-rsa}

One of the most interesting applications of Coppersmith's algorithm is to attack variants of RSA\@.

\subsection{RSA Recap}%
\label{sec:rsa-recap}

The RSA function and cryptosystem~\cite{DBLP:journals/cacm/RivestSA78} (named after its inventors Rivest, Shamir and Adleman) is one of the most widely used public-key encryption and digital signature schemes in practice.

It is important to distinguish between the RSA \emph{function} and an RSA-based cryptosystem.
The RSA function is defined as follows.
Let $N=pq$, where $p$ and $q$ are distinct very large primes (according to current security recommendations, they should be at least $1{,}000$ bits each).
Let $e$ be a public ``encryption exponent'' such that $\gcd(e,\varphi(N))=1$, where $\varphi(N) = \abs{\Z_{N}^{*}} = (p-1)(q-1)$ is the totient function.
The RSA function $f_{e,N} \colon \Z_{N}^{*} \to \Z_{N}^{*}$ is defined by the public values $(N,e)$ as follows:
\begin{equation}
  \label{eq:rsa}
  f_{e,N}(x) := x^{e} \bmod N.
\end{equation}
It is conjectured that, given just $N,e$, and $y=f_{e,N}(x)$ for a uniformly random $x \in \Z_{N}^{*}$, it is computationally infeasible to find $x$.
However, one can efficiently invert the RSA function using some extra ``trapdoor'' information: let~$d$ be the ``decryption exponent'' defined by $e \cdot d=1 \bmod \varphi(N)$, which is efficiently computable given the factorization of~$N$.
Then the inverse of the RSA function is
\begin{equation}
  \label{eq:rsa-inv}
  f_{e,N}^{-1}(y) := y^{d} \bmod N.
\end{equation}
This indeed inverts the function because, for $y=f_{e,N}(x) = x^{e} \bmod N$, we have
\[ y^d=(x^e)^d = (x^{e \cdot d \bmod \varphi(N)}) = x \bmod N, \] where the first inequality holds because the order of the group~$\Z_{N}^{*}$ is~$\varphi(N)$, and by Euler's Theorem.
It follows that~$f_{e,N}$ is a bijection (also called a \emph{permutation}) on~$\Z_{N}^{*}$.

\subsection{Low-Exponent Attacks}%
\label{sec:low-exponent-attacks}

Because exponentiation modulo huge integers is somewhat computationally expensive, many early proposals advocated using encryption exponents as small as $e=3$ with RSA, due to some significant efficiency benefits.\footnote{Note that one cannot use $e=2$ (or any other even $e$) because it is never coprime with $\varphi(N)$, which is also even.}
For example, using an encryption exponent of the form $e=2^{k}+1$, one can compute $x^{e}$ using only $k+1$ modular multiplications via the standard ``repeated squaring'' method.
Moreover, the basic RSA function still appears to be hard to invert (on uniformly random $y \in \Z_{N}^{*}$) for small~$e$.

Like any other deterministic public-key encryption scheme, the RSA function itself is not a secure cryptosystem.
This is because if an attacker had a guess about the message in a ciphertext $c$ (say, because the number of possible messages was small), it could easily test whether its guess was correct by encrypting the message and checking whether the resulting ciphertext matched $c$.
Therefore, one needs to use a \emph{randomized} encryption algorithm.
The most obvious way to randomize the RSA function is to append some random ``padding'' bits to the end of the message before encrypting.
However, devising a secure padding scheme is quite difficult and subtle, due to the existence of clever attacks using tools like LLL\@.
Here we describe one such attack: when~$e$ is small and the padding is not long enough, the attack efficiently recovers an encrypted message~$M$ given just two encryptions of~$M$ with different paddings.\footnote{In real-life applications it is quite common to encrypt the same message more than once, e.g., via common protocol headers or retransmission.}

We start with a simple ``related-message attack'' of~\cite{DBLP:conf/eurocrypt/CoppersmithFPR96}, which is interesting in its own right and will also be a key component of the padding attack.
Note that this attack is on the deterministic RSA function.

\begin{lemma}%
  \label{lem:related-msg}
  Let $e=3$ and $M_1, M_2 \in \Z_{N}^*$ satisfy $M_1=\ell(M_2) \bmod N$, where $\ell(M)=aM+b$ for some $a,b\neq 0$ is a known linear function.
  Then one can efficiently recover $M_1, M_2$ from $C_1=M_1^{e} \bmod N$ and $C_2=M_2^e \bmod N$ (and the other public information $N,e,a,b$).\footnote{It turns out that theorem is ``usually'' true even for $e>3$, but there are rare exceptions.}
\end{lemma}

\begin{proof}
  Define polynomials $g_1(x)=\ell(x)^{e}-C_1, g_2(x)=x^{e}-C_2 \in \Z_{N}[x]$.
  Notice that~$M_2$ is a root of both~$g_1$ and~$g_2$, and thus $(x-M_2)$ is a common divisor of~$g_1$ and~$g_2$ (here is where we need that $a \neq 0$).
  If it is in fact the greatest common divisor (i.e., the common divisor of largest degree), then we can find it using Euclid's algorithm.\footnote{Strictly speaking, Euclid's algorithm would normally require $\Z_{N}$ to be a field, which it is not.
    However, if Euclid's algorithm fails in this setting then it reveals the factorization of~$N$ as a side effect, which lets us compute the decryption exponent and the messages.}
  We show next that $(x-M_{2})$ is indeed the greatest common divisor.

  First observe that for any $C \in \Z_{N}^*$, the polynomial $x^3-C$ has only one root modulo $N$, because the RSA function is a bijection.
  As a result, $g_2(x)=x^3-C_2=(x-M_{2})(x^2+\beta_1x+\beta_0)$, where the quadratic term is irreducible, so $\gcd(g_{1}, g_{2})$ is either $(x-M_{2})$ or $g_{2}$.
  Since $b \neq 0$, we have that $g_2(x) \nmid g_1(x)$, and therefore the greatest common divisor is indeed $x-M_{2}$.
\end{proof}

We now consider a potential padding method: to encrypt a message $M$, one appends $m$ uniformly random bits $r \in \bit^{m}$ (for some publicly known value of $m$), and applies the RSA function:
\[ C=f_{N,e}(M \| r).
\] (Upon decryption, the padding bits are ignored.)
Mathematically, this corresponds to transforming~$M$ to $2^{m} \cdot M + r$ for uniformly random $r \in \set{0, \ldots, 2^{m}-1}$.
Note that~$M$ must have few enough bits, and~$m$ must be small enough, so that the result can be interpreted as an element of $\Z_{N}^{*}$ and unambiguously represents~$M$.

The following theorem of~\cite{DBLP:journals/joc/Coppersmith97} shows that if the pad length is too short (as determined by the size of~$e$), then it is possible to efficiently recover the message~$M$ from two distinct encryptions of it.
Notice that for $e=3$, a pad length of $n/9 \approx 2{,}000/9 \approx 222$ is large enough to prevent any repeated pad values with overwhelming probability, yet it still provides essentially no security.

\begin{theorem}%
  \label{thm:small-e-padding}
  Let $N$ have bit length $n$, let $e=3$, and let $m \leq \floor{n/e^2}$ be the padding length (in bits).
  Given two encryptions $C_1, C_2$ of the same message~$M$ with arbitrary distinct pads $r_{1}, r_{2} \in \set{0, \ldots, 2^{m}-1}$, one can efficiently recover~$M$.
\end{theorem}

\begin{proof}
  We have that $M_1 = 2^{m}\cdot M+r_1$ and $M_2=2^{m}\cdot M+r_2$ for some distinct $r_1,r_2 \in \{0,\ldots, 2^{m}-1\}$.
  We define two bivariate polynomials $g_{1}(x,y), g_{2}(x,y) \in \Z_{N}[x]$ as
  \begin{align}
    g_1(x,y) &= x^{e} - C_1 = x^{e}-M_1^{e}, \\
    g_2(x,y) &= (x+y)^{e}-C_2=(x+y)^{e}-M_2^{e} .
  \end{align}
  Essentially, $x$ represents the unknown message, and $y$ represents the unknown pads.
  Since $g_1$ is independent of $y$, we have that $(x=M_1, y = \star)$ is a root of $g_1$ for any value of $y$.
  Similarly, $(x=M_{1}, y=r_2-r_1)$ is a root of $g_2$.

  To take the next step, we need a concept called the \emph{resultant} of two polynomials in a variable $x$, which is defined as the product of all the differences between their respective roots:
  \[ \res_x(p(x),q(x))=\prod_{p(x_0)=q(x_1)=0}(x_0-x_1). \] In our
  setting, we treat the bivariate polynomials $g_{i}(x,y)$ as
  polynomials in $x$ whose coefficients are polynomials in $y$ (i.e.,
  elements of $\Z_{N}[y]$), so $\res_{x}(g_{1}, g_{2})$ is a
  polynomial in~$y$.

  We use a few important facts about the resultant:
  \begin{itemize}[itemsep=0pt]
  \item First, it is clear that $\res_x(p(x),q(x))=0$ when $p,q$ have a common root.
  \item Second, $\res_x(p,q)=\det(\matS_{p,q})$, where $\matS_{p,q}$ is a square $(\deg{p}+\deg{q})$-dimensional matrix called the Sylvester matrix, whose entries are made up of various shifts of the coefficient vectors of $p$ and $q$.
    Therefore, the resultant can be computed efficiently.
  \item Finally, in our setting the $x$-coefficients of~$g_{1}$ are constant (i.e., degree-$0$) polynomials in~$y$, while the $x$-coefficients of~$g_{2}$ are polynomials of degree at most~$e$ in~$y$.
    By definition of the Sylvester matrix, the resultant $h(y) = \res_{x}(g_{1}, g_{2})$ has degree at most~$e^{2}$ in~$y$.
  \end{itemize}

  We claim that $\Delta=r_2-r_1 \neq 0$, which has absolute value $\abs{\Delta} \leq 2^{m} < N^{1/e^{2}}$, is a root of the resultant $h(y)=\res_x(g_1,g_2)$.
  This is because the univariate polynomials $g_1(x,\Delta)$ and $g_2(x,\Delta)$ have a common root $x=M_{1}$.

  Armed with this information, our attack proceeds as follows.
  We construct the polynomials $g_1, g_2$, and compute the resultant $h(y)=\res_x(g_1,g_2)$.
  Then $\deg(h(y)) \leq e^2$, and we know that $h(y)$ has $\Delta=r_2-r_1 \neq 0$ as a root modulo~$N$.
  We run Coppersmith's algorithm on $h(y)$, and since $\abs{\Delta} \leq 2^{m} < N^{1/e^2}$, we get a polynomial-length list of small roots that contains~$\Delta$.
  Trying each element of the list as a candidate for~$\Delta$, we have a known (candidate) linear function $\ell(M) = M - \Delta$ where $M_{1} = \ell(M_{2})$, and we can run the related message attack from \cref{lem:related-msg}.\footnote{Note that we have rigorously proved \cref{lem:related-msg} only for the case $e=3$, but the attack works as long as the associated algorithm actually succeeds.}
  One of these runs involves the correct value of~$\Delta$ and thus reveals $M_{1}, M_{2}$, and we can confirm which run does so by re-encrypting and checking against $C_{1}, C_{2}$.
\end{proof}

The above is just one of countless lattice attacks against classical number-theoretic cryptography, not limited to variants of RSA:
\begin{itemize}[itemsep=0pt]
\item Small decryption exponent~$d$: for more efficient decryption, a natural idea is to use a relatively small decryption exponent~$d$ (with whatever~$e$ it induces).
  However, this can be completely insecure: to date, the best known attack efficiently recovers~$d$ if it is less than $N^{0.292}$~\cite{DBLP:journals/tit/BonehD00}.
  This uses a bivariate version of Coppersmith~\cite{DBLP:conf/eurocrypt/Coppersmith96a} that lacks a rigorous proof of correctness in general, but works well empirically.
  Important open questions are whether $d < N^{1/2-\varepsilon}$ is efficiently attackable (it is conjectured to be, but no effective attack has been found in the more than 25 years since the conjecture was made), and whether there are rigorously provable variants of Coppersmith for bivariate or multivariate polynomials.

\item Partial secret key exposure: when certain bits of~$d$ or the factors $p,q$ of~$N$ are known to the attacker, it is often possible to efficiently recover them completely.
  See, e.g.,~\cite{DBLP:conf/eurocrypt/Coppersmith96a,DBLP:conf/asiacrypt/BonehDF98}.

\item Larger powers of prime factors: for integers of the form $N=p^{r} q$ for larger $r > 1$, there is a specialized lattice-based attack~\cite{DBLP:conf/crypto/BonehDH99}.
  The performance improves as~$r$ increases, resulting in a polynomial-time attack when~$r$ is on the order of~$\log p$.

\item Partially known or biased `nonces': many signature schemes that rely on the hardness of discrete logarithms, such as the (Elliptic Curve) Digital Signature Algorithm, or (EC)DSA, generate a secret random `nonce' value during signing.
  If partial information about the nonce is leaked, or if nonces are non-uniformly biased, then there are effective lattice attacks that recover the secret signing key.
  See, e.g.,~\cite{DBLP:journals/dcc/Howgrave-GrahamS01,DBLP:journals/joc/NguyenS02,DBLP:journals/dcc/NguyenS03}.
\end{itemize}

\bibliography{common/lattices,common/crypto}
\bibliographystyle{common/alphaabbrvprelim}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
